{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Prediction Results\n",
    "\n",
    "This notebook loads and analyzes the prediction results saved from `demo_working.py`. It compares the performance of the standard model and the Test-Time Trained (TTT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Apply a common style for plots\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except OSError:\n",
    "    print(\"Seaborn style 'seaborn-v0_8-whitegrid' not found, using default matplotlib style.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the results file (assuming notebook is in project root)\n",
    "results_path = 'results/prediction_results.npy'\n",
    "\n",
    "# Load the results\n",
    "results = np.load(results_path, allow_pickle=True).item()\n",
    "\n",
    "# Extract data components\n",
    "y_true = results['true']\n",
    "std_pred = results['standard']['pred']\n",
    "std_metrics = results['standard']['metrics']\n",
    "ttt_pred = results['ttt']['pred']\n",
    "ttt_metrics = results['ttt']['metrics']\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Shape of y_true: {y_true.shape}\")\n",
    "print(f\"Shape of std_pred: {std_pred.shape}\")\n",
    "print(f\"Shape of ttt_pred: {ttt_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display Stored Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Standard Prediction Metrics ---\")\n",
    "for metric, value in std_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "print(\"\n",
    "--- TTT Prediction Metrics ---\")\n",
    "for metric, value in ttt_metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "print(\"\n",
    "--- Improvement with TTT ---\")\n",
    "for metric_key in ['mse', 'mae', 'rmse']:\n",
    "    if std_metrics[metric_key] != 0: # Avoid division by zero\n",
    "        improvement = (std_metrics[metric_key] - ttt_metrics[metric_key]) / std_metrics[metric_key] * 100\n",
    "        print(f\"Improvement in {metric_key.upper()}: {improvement:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Improvement in {metric_key.upper()}: N/A (standard metric is zero)\")\n",
    "\n",
    "# R² improvement (higher is better)\n",
    "if abs(std_metrics['r2']) > 1e-9: # Avoid division by near-zero or zero\n",
    "    r2_improvement = (ttt_metrics['r2'] - std_metrics['r2']) / abs(std_metrics['r2']) * 100\n",
    "    print(f\"Improvement in R²: {r2_improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"Improvement in R²: N/A (standard R² is near zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scatter Plots: True vs. Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Standard Prediction Plot\n",
    "ax1.scatter(y_true, std_pred, alpha=0.6, edgecolors='w', linewidth=0.5, label='Predictions')\n",
    "ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2, label='Ideal (y=x)')\n",
    "ax1.set_title('Standard Prediction Performance', fontsize=14)\n",
    "ax1.set_xlabel('True Values', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Values', fontsize=12)\n",
    "std_metrics_text = f\"R²: {std_metrics['r2']:.3f}\\nMAE: {std_metrics['mae']:.3f}\\nMSE: {std_metrics['mse']:.3f}\"\n",
    "ax1.text(0.05, 0.95, std_metrics_text, transform=ax1.transAxes, fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.8))\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# TTT Prediction Plot\n",
    "ax2.scatter(y_true, ttt_pred, alpha=0.6, edgecolors='w', linewidth=0.5, label='Predictions')\n",
    "ax2.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2, label='Ideal (y=x)')\n",
    "ax2.set_title('TTT Prediction Performance', fontsize=14)\n",
    "ax2.set_xlabel('True Values', fontsize=12)\n",
    "ax2.set_ylabel('Predicted Values', fontsize=12)\n",
    "ttt_metrics_text = f\"R²: {ttt_metrics['r2']:.3f}\\nMAE: {ttt_metrics['mae']:.3f}\\nMSE: {ttt_metrics['mse']:.3f}\"\n",
    "ax2.text(0.05, 0.95, ttt_metrics_text, transform=ax2.transAxes, fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.8))\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
    "fig.suptitle('Comparison of Prediction Accuracy: Standard vs. TTT', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Histograms of Residuals (True - Predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_residuals = y_true.flatten() - std_pred.flatten()\n",
    "ttt_residuals = y_true.flatten() - ttt_pred.flatten()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n",
    "\n",
    "# Standard Residuals\n",
    "ax1.hist(std_residuals, bins=30, alpha=0.75, color='cornflowerblue', edgecolor='black')\n",
    "ax1.axvline(std_residuals.mean(), color='red', linestyle='dashed', linewidth=2, label=f'Mean: {std_residuals.mean():.3f}')\n",
    "ax1.axvline(np.median(std_residuals), color='darkorange', linestyle='dashed', linewidth=2, label=f'Median: {np.median(std_residuals):.3f}')\n",
    "ax1.set_title('Residuals Distribution (Standard Prediction)', fontsize=14)\n",
    "ax1.set_xlabel('Residual (True - Predicted)', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# TTT Residuals\n",
    "ax2.hist(ttt_residuals, bins=30, alpha=0.75, color='salmon', edgecolor='black')\n",
    "ax2.axvline(ttt_residuals.mean(), color='red', linestyle='dashed', linewidth=2, label=f'Mean: {ttt_residuals.mean():.3f}')\n",
    "ax2.axvline(np.median(ttt_residuals), color='darkorange', linestyle='dashed', linewidth=2, label=f'Median: {np.median(ttt_residuals):.3f}')\n",
    "ax2.set_title('Residuals Distribution (TTT Prediction)', fontsize=14)\n",
    "ax2.set_xlabel('Residual (True - Predicted)', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout\n",
    "fig.suptitle('Distribution of Prediction Residuals: Standard vs. TTT', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Standard Residuals - Mean: {std_residuals.mean():.4f}, Std Dev: {std_residuals.std():.4f}, Median: {np.median(std_residuals):.4f}\")\n",
    "print(f\"TTT Residuals      - Mean: {ttt_residuals.mean():.4f}, Std Dev: {ttt_residuals.std():.4f}, Median: {np.median(ttt_residuals):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
